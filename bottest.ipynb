{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnusnath447/BYOB/blob/main/bottest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch python-docx scipy accelerate requests"
      ],
      "metadata": {
        "id": "32-LnpvRbcX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from docx import Document\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Function to extract text from a .docx file, including tables\n",
        "def extract_text_from_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    data_rows = []\n",
        "\n",
        "    # Extract text from tables\n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]\n",
        "            if row_text:\n",
        "                data_rows.append(\" | \".join(row_text))  # Store rows as strings\n",
        "\n",
        "    return data_rows\n",
        "\n",
        "# Read document and extract rows\n",
        "file_path = \"alerts.docx\"\n",
        "table_rows = extract_text_from_docx(file_path)\n",
        "\n",
        "# Function to get embedding\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # Mean pooling\n",
        "    return embedding\n",
        "\n",
        "# Embed each table row\n",
        "row_embeddings = {row: get_embedding(row) for row in table_rows}\n",
        "\n",
        "# Embed the query \"High CPU Usage\"\n",
        "query_text = \"Kubernetes Pod CrashLoop\"\n",
        "query_embedding = get_embedding(query_text)\n",
        "\n",
        "# Find the most similar row using cosine similarity\n",
        "best_match = min(row_embeddings.items(), key=lambda x: cosine(query_embedding, x[1]))\n",
        "\n",
        "# Print the most relevant row\n",
        "# print(\"Best match for 'Disk Space Low':\")\n",
        "# print(best_match[0])\n"
      ],
      "metadata": {
        "id": "1VgEkHRc7rtL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Formating using Rule-based"
      ],
      "metadata": {
        "id": "2RlQbyC39Xvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def format_resolution_steps(text):\n",
        "    # Extract only the resolution steps part\n",
        "    if \"|\" in text:\n",
        "        resolution_part = text.split(\"|\")[-1].strip()  # Get the last part after '|'\n",
        "    else:\n",
        "        resolution_part = text.strip()\n",
        "\n",
        "    # Split steps using numbers like \"1.\", \"2.\", etc.\n",
        "    steps = re.split(r'\\d+\\.\\s*', resolution_part)\n",
        "    steps = [step.strip() for step in steps if step.strip()]  # Remove empty elements\n",
        "\n",
        "    # Format output\n",
        "    formatted_output = \"Resolution steps:\\n\"\n",
        "    for i, step in enumerate(steps, start=1):\n",
        "        formatted_output += f\"Step {i}: {step}\\n\"\n",
        "\n",
        "    return formatted_output.strip()\n",
        "\n",
        "# Example output from retrieval\n",
        "retrieved_text = best_match[0]\n",
        "\n",
        "# Format the retrieved output\n",
        "formatted_result = format_resolution_steps(retrieved_text)\n",
        "print(formatted_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnMRgQRm9rSE",
        "outputId": "d720dcea-a2e4-4d29-b483-c55241a4aa53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolution steps:\n",
            "Step 1: Check pod logs for errors.\n",
            "Step 2: Restart the pod manually.\n",
            "Step 3: Investigate root cause and fix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fomating using downloaded model ( no use )"
      ],
      "metadata": {
        "id": "-n3grXh_9O09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load a small model (TinyLlama-1.1B)\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
        "\n",
        "# Function to format alert resolution steps\n",
        "def format_alert(prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    output = model.generate(input_ids, max_new_tokens=100)\n",
        "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Trim output to remove unwanted text\n",
        "    result = result.strip().split(\"\\n\")\n",
        "    result = [line for line in result if line.startswith(\"-\") or line.startswith(\"*\")]  # Keep only bullet points\n",
        "    return \"\\n\".join(result)\n",
        "\n",
        "# Example input (retrieved alert data)\n",
        "retrieved_text = best_match[0]\n",
        "\n",
        "# Improved prompt\n",
        "prompt = f\"\"\"\n",
        "Extract only the resolution steps from the following alert and format them as bullet points. Do not add any extra text.\n",
        "\n",
        "Alert: {retrieved_text}\n",
        "\"\"\"\n",
        "\n",
        "# Get formatted response\n",
        "formatted_output = format_alert(prompt)\n",
        "print(formatted_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnzpXb9ly1cp",
        "outputId": "c5ef2941-acd2-49af-d9c1-8bf0d0008cf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Model to format text , Together API"
      ],
      "metadata": {
        "id": "7UyIvTdJ8R-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "TOGETHER_API_KEY = \"c05c3f10cb1601a44b6761a7cc69899c69f8728b4e0ea4c70e4fb407957f0a51\"  # Get from https://www.together.ai/\n",
        "\n",
        "def format_alert_together(alert_text):\n",
        "    url = \"https://api.together.xyz/v1/completions\"\n",
        "    headers = {\"Authorization\": f\"Bearer {TOGETHER_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Extract only the resolution steps from the following alert and format them as bullet points. Do not add any extra text.\n",
        "\n",
        "    Alert: {alert_text}\n",
        "    \"\"\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 100,\n",
        "        \"temperature\": 0.2\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    response_json = response.json()\n",
        "\n",
        "    #Print the full response for debugging\n",
        "    print(\"API Response:\", response_json)\n",
        "\n",
        "    # Check if 'choices' exists\n",
        "    if \"choices\" in response_json:\n",
        "        return response_json[\"choices\"][0][\"text\"].strip()\n",
        "    else:\n",
        "        return f\"Error: {response_json}\"  # Return the error message\n",
        "\n",
        "# Example Input\n",
        "retrieved_text = best_match[0]\n",
        "\n",
        "formatted_output = format_alert_together(retrieved_text)\n",
        "print(formatted_output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkuIUEW82zPF",
        "outputId": "4ee7938f-7af5-49ab-a36e-5aa9bdffeceb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Response: {'id': '914445c36bc22b2a', 'object': 'text.completion', 'created': 1739947300, 'model': 'meta-llama/Llama-3.3-70B-Instruct-Turbo', 'prompt': [], 'choices': [{'text': ' becomes:\\n    * Check pod logs for errors.\\n    * Restart the pod manually.\\n    * Investigate root cause and fix.', 'finish_reason': 'eos', 'seed': 8123650474972279000, 'logprobs': None, 'index': 0}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 27, 'total_tokens': 107}}\n",
            "becomes:\n",
            "    * Check pod logs for errors.\n",
            "    * Restart the pod manually.\n",
            "    * Investigate root cause and fix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a-0NdlIo2z0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}